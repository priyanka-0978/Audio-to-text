# -*- coding: utf-8 -*-
"""Internship_assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vq-zBVZmU_OVdAdRJ6mXC9KX7IZtrUFp
"""

#Importing the libraries
!pip install transformers

!pip install accelerate

#Checking for the device availability if GPU is available use it else use CPU
import torch
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
device = "cuda:0" if torch.cuda.is_available() else "cpu"
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

#Using the format of code provided by hugging face for Whisper model to load the model, pass the model_id
model_id = "openai/whisper-large-v3"

model = AutoModelForSpeechSeq2Seq.from_pretrained(
    model_id, torch_dtype=torch_dtype, use_safetensors=True
)
model.to(device)

processor = AutoProcessor.from_pretrained(model_id)

#initialising the pipeline to do ASR
pipe = pipeline(
    "automatic-speech-recognition",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    max_new_tokens=128,
    chunk_length_s=30,
    batch_size=16,
    torch_dtype=torch_dtype,
    device=device,
)

#Applying the pre-trained model to transcribe all the audios(1816) and save it in a list
import os
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

folder_path = '/content/drive/MyDrive/common_voice_test'
transcriptions = []
file_list = os.listdir(folder_path)

file_list.sort()

for file_name in file_list:
    audio_path = os.path.join(folder_path, file_name)
    result = pipe(audio_path)
    transcription = result["text"]
    transcriptions.append(transcription)

#Defining function to load dataset of ground truth
import pandas as pd
def load_transcripted_data(filepath, cols):
    df = pd.read_csv(filepath, header=None)
    df.columns = cols
    return df
df=load_transcripted_data('/content/trans.txt', cols=["Marathi_text"])
#Modifying the given transcripted script by removing audio_id to perform wer
import re
new_sentences = []
for i in range(len(df)):
  lines=df["Marathi_text"][i]
  new_sentence = re.sub(r"common_voice_mr_\d+\.wav\s*", "", lines)
  new_sentences.append(new_sentence)
df=pd.DataFrame(new_sentences,columns=["marathi_text"])
#Saving it to csv file so can download it back easily when requiered to perform wer
df.to_csv("marathi_script.csv",index=False)

#Importing the ground truth to perform Wer
import pandas as pd
ground_truth=pd.read_csv("/content/marathi_script (1).csv")

#Converting the ground truth to a list
actual_transcription=[]
for i in range(len(ground_truth)):
  actual_transcription.append(ground_truth["marathi_text"][i])

#Defining a function to calculate word error rate
def calculate_accuracy(reference, hypothesis):
    reference_words = reference.split()
    hypothesis_words = hypothesis.split()
    substitutions = sum(1 for ref, hyp in zip(reference_words, hypothesis_words) if ref != hyp)
    deletions = len(reference_words) - len(hypothesis_words)
    insertions = len(hypothesis_words) - len(reference_words)
    total_words = len(reference_words)
    wer = (substitutions + deletions + insertions) / total_words
    return wer

reference_list = transcriptions
hypothesis_list = actual_transcription
wer_list = []
for reference, hypothesis in zip(reference_list, hypothesis_list):
    wer = calculate_accuracy(reference, hypothesis)
    wer_list.append(wer)
print(wer_list)

#Average Word Error Rate
print(sum(wer_list)/len(actual_transcription)*100)